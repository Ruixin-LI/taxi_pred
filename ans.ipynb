{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2182e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f546755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/70], Step [500/1000], Loss: 11.7513\n",
      "Epoch [1/70], Step [1000/1000], Loss: 9.0430\n",
      "Epoch [2/70], Step [500/1000], Loss: 14.9804\n",
      "Epoch [2/70], Step [1000/1000], Loss: 11.0783\n",
      "Epoch [3/70], Step [500/1000], Loss: 7.1718\n",
      "Epoch [3/70], Step [1000/1000], Loss: 10.7226\n",
      "Epoch [4/70], Step [500/1000], Loss: 5.9076\n",
      "Epoch [4/70], Step [1000/1000], Loss: 7.7332\n",
      "Epoch [5/70], Step [500/1000], Loss: 6.3698\n",
      "Epoch [5/70], Step [1000/1000], Loss: 7.1084\n",
      "Epoch [6/70], Step [500/1000], Loss: 6.9797\n",
      "Epoch [6/70], Step [1000/1000], Loss: 5.9480\n",
      "Epoch [7/70], Step [500/1000], Loss: 6.2791\n",
      "Epoch [7/70], Step [1000/1000], Loss: 7.3541\n",
      "Epoch [8/70], Step [500/1000], Loss: 7.5475\n",
      "Epoch [8/70], Step [1000/1000], Loss: 6.1868\n",
      "Epoch [9/70], Step [500/1000], Loss: 7.8119\n",
      "Epoch [9/70], Step [1000/1000], Loss: 7.5621\n",
      "Epoch [10/70], Step [500/1000], Loss: 8.1741\n",
      "Epoch [10/70], Step [1000/1000], Loss: 7.2734\n",
      "Epoch [11/70], Step [500/1000], Loss: 6.4317\n",
      "Epoch [11/70], Step [1000/1000], Loss: 4.6268\n",
      "Epoch [12/70], Step [500/1000], Loss: 6.6804\n",
      "Epoch [12/70], Step [1000/1000], Loss: 7.6099\n",
      "Epoch [13/70], Step [500/1000], Loss: 6.2464\n",
      "Epoch [13/70], Step [1000/1000], Loss: 6.1295\n",
      "Epoch [14/70], Step [500/1000], Loss: 4.9833\n",
      "Epoch [14/70], Step [1000/1000], Loss: 7.7342\n",
      "Epoch [15/70], Step [500/1000], Loss: 5.7477\n",
      "Epoch [15/70], Step [1000/1000], Loss: 5.7549\n",
      "Epoch [16/70], Step [500/1000], Loss: 9.3769\n",
      "Epoch [16/70], Step [1000/1000], Loss: 6.7467\n",
      "Epoch [17/70], Step [500/1000], Loss: 5.9452\n",
      "Epoch [17/70], Step [1000/1000], Loss: 6.3776\n",
      "Epoch [18/70], Step [500/1000], Loss: 5.7188\n",
      "Epoch [18/70], Step [1000/1000], Loss: 8.1527\n",
      "Epoch [19/70], Step [500/1000], Loss: 9.3191\n",
      "Epoch [19/70], Step [1000/1000], Loss: 7.9003\n",
      "Epoch [20/70], Step [500/1000], Loss: 6.5678\n",
      "Epoch [20/70], Step [1000/1000], Loss: 5.6738\n",
      "Epoch [21/70], Step [500/1000], Loss: 6.0775\n",
      "Epoch [21/70], Step [1000/1000], Loss: 7.9797\n",
      "Epoch [22/70], Step [500/1000], Loss: 5.3756\n",
      "Epoch [22/70], Step [1000/1000], Loss: 6.1474\n",
      "Epoch [23/70], Step [500/1000], Loss: 5.5117\n",
      "Epoch [23/70], Step [1000/1000], Loss: 5.3745\n",
      "Epoch [24/70], Step [500/1000], Loss: 5.3385\n",
      "Epoch [24/70], Step [1000/1000], Loss: 10.5628\n",
      "Epoch [25/70], Step [500/1000], Loss: 6.0911\n",
      "Epoch [25/70], Step [1000/1000], Loss: 11.9635\n",
      "Epoch [26/70], Step [500/1000], Loss: 4.2156\n",
      "Epoch [26/70], Step [1000/1000], Loss: 6.6502\n",
      "Epoch [27/70], Step [500/1000], Loss: 7.2738\n",
      "Epoch [27/70], Step [1000/1000], Loss: 6.1578\n",
      "Epoch [28/70], Step [500/1000], Loss: 6.1712\n",
      "Epoch [28/70], Step [1000/1000], Loss: 7.4183\n",
      "Epoch [29/70], Step [500/1000], Loss: 6.8562\n",
      "Epoch [29/70], Step [1000/1000], Loss: 7.3419\n",
      "Epoch [30/70], Step [500/1000], Loss: 8.6022\n",
      "Epoch [30/70], Step [1000/1000], Loss: 8.3823\n",
      "Epoch [31/70], Step [500/1000], Loss: 11.9249\n",
      "Epoch [31/70], Step [1000/1000], Loss: 5.7763\n",
      "Epoch [32/70], Step [500/1000], Loss: 4.8565\n",
      "Epoch [32/70], Step [1000/1000], Loss: 5.0513\n",
      "Epoch [33/70], Step [500/1000], Loss: 8.6592\n",
      "Epoch [33/70], Step [1000/1000], Loss: 13.2158\n",
      "Epoch [34/70], Step [500/1000], Loss: 6.4412\n",
      "Epoch [34/70], Step [1000/1000], Loss: 5.7820\n",
      "Epoch [35/70], Step [500/1000], Loss: 6.2732\n",
      "Epoch [35/70], Step [1000/1000], Loss: 5.2436\n",
      "Epoch [36/70], Step [500/1000], Loss: 7.5399\n",
      "Epoch [36/70], Step [1000/1000], Loss: 5.0694\n",
      "Epoch [37/70], Step [500/1000], Loss: 8.0812\n",
      "Epoch [37/70], Step [1000/1000], Loss: 4.8194\n",
      "Epoch [38/70], Step [500/1000], Loss: 5.6562\n",
      "Epoch [38/70], Step [1000/1000], Loss: 7.8919\n",
      "Epoch [39/70], Step [500/1000], Loss: 4.3509\n",
      "Epoch [39/70], Step [1000/1000], Loss: 3.0580\n",
      "Epoch [40/70], Step [500/1000], Loss: 7.5420\n",
      "Epoch [40/70], Step [1000/1000], Loss: 11.8239\n",
      "Epoch [41/70], Step [500/1000], Loss: 5.6355\n",
      "Epoch [41/70], Step [1000/1000], Loss: 10.7035\n",
      "Epoch [42/70], Step [500/1000], Loss: 7.4552\n",
      "Epoch [42/70], Step [1000/1000], Loss: 7.0254\n",
      "Epoch [43/70], Step [500/1000], Loss: 6.3570\n",
      "Epoch [43/70], Step [1000/1000], Loss: 4.6236\n",
      "Epoch [44/70], Step [500/1000], Loss: 4.7435\n",
      "Epoch [44/70], Step [1000/1000], Loss: 5.3894\n",
      "Epoch [45/70], Step [500/1000], Loss: 6.3702\n",
      "Epoch [45/70], Step [1000/1000], Loss: 7.2346\n",
      "Epoch [46/70], Step [500/1000], Loss: 5.3333\n",
      "Epoch [46/70], Step [1000/1000], Loss: 5.6216\n",
      "Epoch [47/70], Step [500/1000], Loss: 8.6547\n",
      "Epoch [47/70], Step [1000/1000], Loss: 4.4692\n",
      "Epoch [48/70], Step [500/1000], Loss: 4.9042\n",
      "Epoch [48/70], Step [1000/1000], Loss: 4.4841\n",
      "Epoch [49/70], Step [500/1000], Loss: 5.5337\n",
      "Epoch [49/70], Step [1000/1000], Loss: 8.1174\n",
      "Epoch [50/70], Step [500/1000], Loss: 4.9846\n",
      "Epoch [50/70], Step [1000/1000], Loss: 5.3071\n",
      "Epoch [51/70], Step [500/1000], Loss: 5.9334\n",
      "Epoch [51/70], Step [1000/1000], Loss: 4.7579\n",
      "Epoch [52/70], Step [500/1000], Loss: 4.2878\n",
      "Epoch [52/70], Step [1000/1000], Loss: 3.8352\n",
      "Epoch [53/70], Step [500/1000], Loss: 5.6285\n",
      "Epoch [53/70], Step [1000/1000], Loss: 5.3403\n",
      "Epoch [54/70], Step [500/1000], Loss: 5.0192\n",
      "Epoch [54/70], Step [1000/1000], Loss: 8.1559\n",
      "Epoch [55/70], Step [500/1000], Loss: 6.7091\n",
      "Epoch [55/70], Step [1000/1000], Loss: 3.7267\n",
      "Epoch [56/70], Step [500/1000], Loss: 4.5844\n",
      "Epoch [56/70], Step [1000/1000], Loss: 6.6637\n",
      "Epoch [57/70], Step [500/1000], Loss: 8.2944\n",
      "Epoch [57/70], Step [1000/1000], Loss: 3.7541\n",
      "Epoch [58/70], Step [500/1000], Loss: 6.2501\n",
      "Epoch [58/70], Step [1000/1000], Loss: 5.6636\n",
      "Epoch [59/70], Step [500/1000], Loss: 4.5958\n",
      "Epoch [59/70], Step [1000/1000], Loss: 4.4341\n",
      "Epoch [60/70], Step [500/1000], Loss: 7.2416\n",
      "Epoch [60/70], Step [1000/1000], Loss: 9.2557\n",
      "Epoch [61/70], Step [500/1000], Loss: 6.3977\n",
      "Epoch [61/70], Step [1000/1000], Loss: 4.0147\n",
      "Epoch [62/70], Step [500/1000], Loss: 6.1408\n",
      "Epoch [62/70], Step [1000/1000], Loss: 5.0131\n",
      "Epoch [63/70], Step [500/1000], Loss: 5.6065\n",
      "Epoch [63/70], Step [1000/1000], Loss: 4.9528\n",
      "Epoch [64/70], Step [500/1000], Loss: 4.9466\n",
      "Epoch [64/70], Step [1000/1000], Loss: 9.0745\n",
      "Epoch [65/70], Step [500/1000], Loss: 3.8268\n",
      "Epoch [65/70], Step [1000/1000], Loss: 5.7932\n",
      "Epoch [66/70], Step [500/1000], Loss: 4.2218\n",
      "Epoch [66/70], Step [1000/1000], Loss: 4.3054\n",
      "Epoch [67/70], Step [500/1000], Loss: 4.5508\n",
      "Epoch [67/70], Step [1000/1000], Loss: 6.7589\n",
      "Epoch [68/70], Step [500/1000], Loss: 5.1034\n",
      "Epoch [68/70], Step [1000/1000], Loss: 5.1070\n",
      "Epoch [69/70], Step [500/1000], Loss: 6.0966\n",
      "Epoch [69/70], Step [1000/1000], Loss: 5.7898\n",
      "Epoch [70/70], Step [500/1000], Loss: 6.8186\n",
      "Epoch [70/70], Step [1000/1000], Loss: 4.6645\n",
      "The Root Main square error is 5.698982633590698\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "#const value\n",
    "result_num = 1#numerical value represting people num\n",
    "feature_num = 49\n",
    "\n",
    "# Hyper-parameter\n",
    "embedding_dim = 10\n",
    "hidden_dim = 16\n",
    "dense_dim = 32\n",
    "n_layers = 1\n",
    "\n",
    "#training config\n",
    "epochs = 70\n",
    "batch_size = 72\n",
    "learning_rate=0.001\n",
    "\n",
    "class TaxiDataset(Dataset):\n",
    "    def __init__(self, x, y, loc, time):\n",
    "        self.features = torch.from_numpy(x)\n",
    "        self.labels = torch.from_numpy(y)\n",
    "        self.locations = torch.from_numpy(loc)\n",
    "        self.times = torch.from_numpy(time)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.features[index], self.labels[index], self.locations[index], self.times[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class TaxiCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "    ):\n",
    "        super(TaxiCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, out_channels=16, kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, out_channels=32, kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(352, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "class TaxiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_size,\n",
    "        embedding_dim=1,\n",
    "        hidden_dim=100,\n",
    "        dense_dim=32,\n",
    "        max_norm=2,\n",
    "        n_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            sequence_size,\n",
    "            embedding_dim,\n",
    "            norm_type=2,\n",
    "            max_norm=max_norm,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=n_layers)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, dense_dim)\n",
    "            nn.ReLU()\n",
    "            nn.Linear(dense_dim, result_num)\n",
    "        )\n",
    "        \n",
    "    def __forward__(self, features, locations, times):\n",
    "        embeds = self.embedding(features)\n",
    "       \n",
    "\"\"\"\n",
    "#try to decrise the demension of the feature sequence\n",
    "class TaxiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_size=feature_num,\n",
    "        hidden_dim=1,\n",
    "        dense_dim=16,\n",
    "        n_layers=1,\n",
    "    ):\n",
    "        super(TaxiRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(feature_size, hidden_dim, batch_first=True, num_layers=n_layers)\n",
    "        self.gru = nn.GRU(feature_size, hidden_dim, batch_first=True, num_layers=n_layers)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(8+2+1, dense_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dense_dim, result_num)\n",
    "        )\n",
    "        \n",
    "    def forward(self, features, locations, times):\n",
    "        #lstm to decrease the demension by reprsenting features with one feature\n",
    "        rnn,(_,_) = self.lstm(features)\n",
    "        #_,rnn = self.gru(features)\n",
    "        squeeze = rnn.squeeze()\n",
    "        times = times.unsqueeze(1)\n",
    "        cat = torch.cat((squeeze, locations, times), axis=1)\n",
    "        output = self.mlp(cat)\n",
    "        return output\n",
    "    \n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#read data\n",
    "train_data = np.load('./train.npz')\n",
    "train_features = train_data['x']#features\n",
    "train_labels = train_data['y']#labels\n",
    "train_locations = train_data['locations']#locations\n",
    "train_times = train_data['times']#times\n",
    "val_data = np.load('./val.npz')\n",
    "val_features = val_data['x']#features\n",
    "val_labels = val_data['y']#labels\n",
    "val_locations = val_data['locations']#locations\n",
    "val_times = val_data['times']#times\n",
    "\n",
    "# dataset\n",
    "train_dataset = TaxiDataset(train_features, train_labels, train_locations, train_times)\n",
    "val_dataset = TaxiDataset(val_features, val_labels, val_locations, val_times)\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# initialize model\n",
    "model = TaxiRNN().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "total_step = len(train_dataloader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (features, labels, locations, times) in enumerate(train_dataloader):\n",
    "        features = features.to(torch.float32).to(device)\n",
    "        labels = labels.to(torch.float32).to(device)\n",
    "        \n",
    "        locations = locations.to(torch.float32).to(device)\n",
    "        times = times.to(torch.float32).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features, locations, times)\n",
    "        loss = torch.sqrt(criterion(outputs, labels))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "all_loss = []\n",
    "with torch.no_grad():\n",
    "    for features, labels, locations, times in val_dataloader:\n",
    "        features = features.to(torch.float32).to(device)\n",
    "        labels = labels.to(torch.float32).to(device)\n",
    "        locations = locations.to(torch.float32).to(device)\n",
    "        times = times.to(torch.float32).to(device)\n",
    "        \n",
    "        outputs = model(features, locations, times)\n",
    "        loss = torch.sqrt(criterion(outputs, labels)).item()\n",
    "        all_loss.append(loss)\n",
    "\n",
    "    print(f'The Root Main square error is {np.mean(all_loss)}')\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "#with open('./time.txt',\"a\") as f:\n",
    "#    for n in times:\n",
    "#        f.write(str(n))\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ddcb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
